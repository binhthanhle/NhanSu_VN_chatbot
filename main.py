# import time
# import streamlit as st
# from langchain.chains import RetrievalQA
# from langchain.llms import HuggingFaceHub
# from langchain.vectorstores import FAISS
# # prompts
# from langchain import PromptTemplate, LLMChain
# import textwrap

# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# def load_translate():
#     model_name = "VietAI/envit5-translation"
#     tokenizer = AutoTokenizer.from_pretrained(model_name)  
#     model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
#     return model, tokenizer


# def translate(text:str):

#     inputs = [f"en: {text}"]

#     outputs = model.generate(tokenizer(inputs, return_tensors="pt", padding=True).input_ids.to('cpu'), max_length=1024) 

#     text_output = (tokenizer.batch_decode(outputs, skip_special_tokens=True))
#     return text_output, outputs


# class CFG:
#     # LLMs
#     model_name = 'mistralai/Mistral-7B-Instruct-v0.1'
#     temperature = 0.5
#     top_p = 0.95
#     repetition_penalty = 1.15
#     do_sample = True
#     max_new_tokens = 1024
#     num_return_sequences=1

#     # splitting
#     split_chunk_size = 800
#     split_overlap = 0
    
#     # embeddings
#     embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'

#     # similar passages
#     k = 5
    
#     # paths
#     Embeddings_path =  './faiss_hp/'


# llm = HuggingFaceHub(
#     repo_id = CFG.model_name,
#     model_kwargs={
#         "max_new_tokens": CFG.max_new_tokens,
#         "temperature": CFG.temperature,
#         "top_p": CFG.top_p,
#         "repetition_penalty": CFG.repetition_penalty,
#         "do_sample": CFG.do_sample,
#         "num_return_sequences": CFG.num_return_sequences        
#     },
#     huggingfacehub_api_token = st.secrets["hugging_api_key"]
# )



# from langchain.embeddings import HuggingFaceInstructEmbeddings
# ### download embeddings model
# embeddings = HuggingFaceInstructEmbeddings(
#     model_name = CFG.embeddings_model_repo,
#     model_kwargs = {"device": "cpu"}
# )

# ### load vector DB embeddings
# vectordb = FAISS.load_local(
#     CFG.Embeddings_path,
#     embeddings
# )

# retriever = vectordb.as_retriever(search_kwargs = {"k": CFG.k, "search_type" : "similarity"})

# # we get the context part by embedding retrieval 
# prompt_template_career = """<s>[INST] I want you to act as a career coach. Given the {question}. 
# Suggest 5 portfolio projects and ideas for upgrading the career path and skills for {question}[/INST]"""

# PROMPT_career = PromptTemplate(
#     template = prompt_template_career,
#     input_variables = ["question"]
# )

# # we get the context part by embedding retrieval 
# prompt_template_hr= """<s>[INST] I want you to act as a Human Resource coach. Given the {question}. 
# Explain the situation of the {question} and provide 5 options to solve and suggest the best solution from the above 5 options with details why need to do it[/INST]"""

# PROMPT_hr = PromptTemplate(
#     template = prompt_template_hr,
#     input_variables = ["question"]
# )


# # qa_chain = RetrievalQA.from_chain_type(
# #     llm = llm,
# #     chain_type = "stuff", # map_reduce, map_rerank, stuff, refine
# #     retriever = retriever, 
# #     chain_type_kwargs = {"prompt": PROMPT_hr},
# #     return_source_documents = True,
# #     verbose = False
# # )

# def wrap_text_preserve_newlines(text, width=700):
#     # Split the input text into lines based on newline characters
#     lines = text.split('\n')

#     # Wrap each line individually
#     wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

#     # Join the wrapped lines back together using newline characters
#     wrapped_text = '\n'.join(wrapped_lines)

#     return wrapped_text


# # def process_llm_response(llm_response):
# #     ans = wrap_text_preserve_newlines(llm_response['result'])
    
# #     sources_used = ' \n \n > '.join(
# #         [
# #             source.metadata['source'].split('/')[-1][:-4] + ' - page: ' + str(source.metadata['page'])
# #             for source in llm_response['source_documents']
# #         ]
# #     )
    
# #     ans = ans + ' \n \n \t Sources: \n >' + sources_used
# #     return ans

# def llm_career(question):
#     llm_chain = LLMChain(prompt=PROMPT_career, llm=llm)
#     answer = llm_chain.run(question=question)
#     return answer.strip()

# def llm_hr(question):
#     llm_chain = LLMChain(prompt=PROMPT_hr, llm=llm)
#     answer = llm_chain.run(question=question)
#     return answer.strip()

# # def llm_ans(query):
# #     start = time.time()
# #     llm_response = qa_chain(query)
# #     ans = process_llm_response(llm_response)
# #     end = time.time()

# #     time_elapsed = int(round(end - start, 0))
# #     time_elapsed_str = f' \n \n Time elapsed: {time_elapsed} s'
# #     return ans.strip() + time_elapsed_str


# # question = st.text_input("Asking question:")
# # submit = st.button('Generate Answer!')

# # st.write("---")

# # if submit or question:
# #     # col1, col2 = st.columns(2)
# #     # with col1:
# #     #     st.subheader("Answer from LLM model (Mistral-7B-Instruct-v0.1):")
# #     #     with st.spinner(text="This may take a moment..."):
# #     #         answer_llm = (llm(f"""<s>[INST] {question} [/INST]""", raw_response=True).strip())
# #     #     st.markdown(answer_llm)
# #     # with col2:
# #     #     st.subheader("Answer from Fine-tunning LLM model:")
# #     #     with st.spinner(text="This may take a moment..."):
# #     #         answer_tune= llm_ans(question)
# #     #     st.markdown(answer_tune)
# #     st.subheader("Answer:")
# #     with st.spinner(text="This may take a moment..."):
# #         answer_tune= llm_ans(question)

# col1, col2 = st.columns(2)
# with col1:
#     st.header("HR Coaching")
#     if "messages" not in st.session_state.keys(): # Initialize the chat message history
#         st.session_state.messages = [
#             {"role": "assistant", "content": "Ask me a question about your difficult situation!"}
#         ]
#     if prompt_for_hr := st.chat_input("Your question"): # Prompt for user input and save to chat history
#         st.session_state.messages.append({"role": "user", "content": prompt_for_hr})

#     for message in st.session_state.messages: # Display the prior chat messages
#         with st.chat_message(message["role"]):
#             st.write(message["content"])

#     # If last message is not from assistant, generate a new response
#     if st.session_state.messages[-1]["role"] != "assistant":
#         with st.chat_message("assistant"):
#             with st.spinner("Thinking..."):
#                 response = llm_hr(prompt_for_hr)
#                 st.write(response)
#                 message = {"role": "assistant", "content": response}
#                 st.session_state.messages.append(message) # Add response to message history


# with col2:
#     st.header("Career Coaching")
#     if "messages" not in st.session_state.keys(): # Initialize the chat message history
#         st.session_state.messages = [
#             {"role": "assistant", "content": "Give me your current role with your experience years and your next role target!"}
#         ]

#     if prompt_career_bot := st.chat_input("Your question"): # Prompt for user input and save to chat history
#         st.session_state.messages.append({"role": "user", "content": prompt_career_bot})

#     for message_c in st.session_state.messages: # Display the prior chat messages
#         with st.chat_message(message_c["role"]):
#             st.write(message_c["content"])

#     # If last message is not from assistant, generate a new response
#     if st.session_state.messages[-1]["role"] != "assistant":
#         with st.chat_message("assistant"):
#             with st.spinner("Thinking..."):
#                 response_c = llm_career(prompt_career_bot)
#                 st.write(response_c)
#                 message = {"role": "assistant", "content": response_c}
#                 st.session_state.messages.append(message) # Add response to message history


import streamlit as st
def set_page_info():
    
    st.set_page_config(layout='wide', page_title='Welcome page', page_icon="üë©‚Äçüë©‚Äçüëß‚Äçüëß")
    st.image("image/banner.png")
    st.title("WELCOME TO NGHE-NHAN-SU-VN")
    st.markdown(
        """
        Ngh·ªÅ nh√¢n s·ª± Vi·ªát Nam ƒë∆∞·ª£c th√†nh l·∫≠p t·ª´ 2019 v·ªõi kh√°t v·ªçng ƒë·ªãnh nghƒ©a l·∫°i Ngh·ªÅ Nh√¢n s·ª± t·∫°i Vi·ªát Nam, 
        chia s·∫ª ki·∫øn th·ª©c, c·∫≠p nh·∫≠t c√°c th√¥ng tin m·ªõi v·ªÅ vi·ªác l√†m, th·ªã tr∆∞·ªùng lao ƒë·ªông 
        v√† nh·∫•t l√† "ƒê·∫°o ƒë·ª©c Nh·ªØng Ng∆∞·ªùi L√†m Ngh·ªÅ Nh√¢n s·ª±".

        Ngo√†i c√°c b√†i vi·∫øt, chia s·∫ª th√¥ng tin th√¨ NNS c≈©ng s·∫Ω l√† ƒë∆°n v·ªã h·ªó tr·ª£, 
        t∆∞ v·∫•n c√°c th√¥ng tin, gi·∫£i ƒë√°p c√°c th·∫Øc m·∫Øc c·ªßa c√°c th√†nh vi√™n tham gia. 
        ƒê·ªìng th·ªùi, trong n·ªó l·ª±c v∆∞∆°n t·ªõi c√°c gi·∫£i ph√°p t·ªët nh·∫•t c≈©ng nh∆∞ c√°c 
        n·ªôi dung chia s·∫ª hi·ªáu qu·∫£, NNS r·∫•t mong nh·∫≠n ƒë∆∞·ª£c s·ª± quan t√¢m, 
        ƒë√≥ng g√≥p v√† chia s·∫ª "th·∫≠t" ƒë·ªÉ ch√∫ng ta c√≥ th·ªÉ c√≥ "k·∫øt n·ªëi nh·ªØng "gi√° tr·ªã th·∫≠t" 
        v·ªõi nhau v√† c√πng nhau.

        Ngh·ªÅ Nh√¢n s·ª± Vi·ªát Nam ch∆∞a nh·∫≠n ƒëƒÉng qu·∫£ng c√°o hay gi·ªõi thi·ªáu d·ªãch v·ª• cho b·∫•t c·ª© ƒë∆°n v·ªã, 
        t·ªï ch·ª©c n√†o. V√¨ v·∫≠y, m·ªçi ng∆∞·ªùi h·∫°n ch·∫ø ƒëƒÉng tin spam ho·∫∑c seeding c√≥ ch·ªß ƒë√≠ch. 
        Ngo√†i ra, NNS c≈©ng ch∆∞a duy·ªát c√°c ƒëƒÉng tuy·ªÉn d·ª•ng nh·∫±m ƒë·∫£m b√°o t√≠nh th·ªëng nh·∫•t 
        c·ªßa c√°c n·ªôi dung v√† hi·ªáu qu·∫£ th√¥ng tin truy·ªÅn t·∫£i. Mong c√°c th√†nh vi√™n m·ªõi c√πng h·ªó tr·ª£ v√† h·ª£p t√°c.

        ƒê·ªÉ t√¨m c√°c t√†i li·ªáu ho·∫∑c th√¥ng tin li√™n quan, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ s·ª≠ d·ª•ng t·ª´ kho√° chuy√™n m√¥n. 

        Tr∆∞·ªùng h·ª£p mong mu·ªën h·ª£p t√°c ho·∫∑c ƒë·ªÅ ngh·ªã cung c·∫•p d·ªãch v·ª•, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ tham kh·∫£o c√°c th√¥ng tin nh∆∞ sau:

        I - T∆∞ v·∫•n v√† d·ªãch v·ª• d√†nh cho T·ªï ch·ª©c

            1- Trung t√¢m ƒë√°nh gi√° nƒÉng l·ª±c (Assessment Center): ƒê√°nh gi√° nƒÉng l·ª±c ·ª©ng vi√™n, nh√¢n vi√™n v√† x√¢y d·ª±ng l·ªô tr√¨nh ph√°t tri·ªÉn ngh·ªÅ nghi·ªáp
            2- Thi·∫øt k·∫ø, ƒê√†o t·∫°o v√† chuy·ªÉn giao nƒÉng l·ª±c Nh√¢n s·ª±: Thu h√∫t t√†i nƒÉng, H·ªçc t·∫≠p v√† ph√°t tri·ªÉn, Ph√°t tri·ªÉn t·ªï ch·ª©c, Qu·∫£n tr·ªã hi·ªáu su·∫•t, Ch·∫ø ƒë·ªô ch√≠nh s√°ch (Total Rewards), Khung nƒÉng l·ª±c, VƒÉn ho√° doanh nghi·ªáp, Truy·ªÅn th√¥ng n·ªôi b·ªô, Tr·∫£i nghi·ªám nh√¢n vi√™n
            3- X√¢y d·ª±ng th∆∞∆°ng hi·ªáu NTD (EB), Truy·ªÅn th√¥ng Th∆∞∆°ng hi·ªáu doanh nghi·ªáp
            4- Cung c·∫•p chuy√™n gia, h·ª£p t√°c v√† k·∫øt n·ªëi c√°c doanh nghi·ªáp, ƒë·ªëi t√°c chi·∫øn l∆∞·ª£c
            5- Nghi√™n c·ª©u, ƒëo l∆∞·ªùng s·ª©c kho·∫ª v·∫≠n h√†nh doanh nghi·ªáp, ƒê·ªôi ng≈© L√£nh ƒë·∫°o Qu·∫£n l√Ω.

        II - T∆∞ v·∫•n v√† d·ªãch v·ª• d√†nh cho C√° nh√¢n

            1- Ph√°t tri·ªÉn nƒÉng l·ª±c, ngh·ªÅ nghi·ªáp v√† gi·ªõi thi·ªáu vi·ªác l√†m tr·ªçn g√≥i d√†nh cho c√° nh√¢n
            2- X√¢y d·ª±ng v√† ph√°t tri·ªÉn th∆∞∆°ng hi·ªáu C√° nh√¢n
            3- X√¢y d·ª±ng v√† ph√°t tri·ªÉn nƒÉng l·ª±c d√†nh cho Chuy√™n gia
            4- ƒê√†o t·∫°o chuy√™n m√¥n v·ªÅ Nh√¢n s·ª±, ph√°t tri·ªÉn CƒÉng l·ª±c Chuy√™n s√¢u
            5- ƒê√†o t·∫°o K·ªπ nƒÉng Qu·∫£n l√Ω, L√£nh ƒë·∫°o hi·ªáu qu·∫£
            6- H∆∞·ªõng d·∫´n, nh∆∞·ª£ng quy·ªÅn Kinh doanh v√† d·ªãch v·ª• d√†nh cho c√° nh√¢n gi√∫p c·∫£i thi·ªán thu nh·∫≠p v√† ph√°t tri·ªÉn chuy√™n m√¥n Nh√¢n s·ª±, Ngh·ªÅ nghi·ªáp.
        
        Tr∆∞·ªùng h·ª£p b·∫°n c·∫ßn h·ªó tr·ª£ v·∫•n ƒë·ªÅ g√¨, c√≥ th·ªÉ ƒë·ªÉ l·∫°i th√¥ng tin t·∫°i ƒë√¢y: https://bit.ly/nns247
        
        M·ªôt l·∫ßn n·ªØa c·∫£m ∆°n v√† ch√∫c c√°c th√†nh vi√™n m·ªõi th·∫≠t nhi·ªÅu ni·ªÅm vui v√† ƒë·ªìng h√†nh c√πng NNS trong th·ªùi gian d√†i.
        Tr√¢n tr·ªçng,


        ********************************
        For more details, please contact: 
        
        Admissions and Consultation Department:
        
        - :telephone_receiver:  0984013214 - 0948013214 

        - :incoming_envelope:   chung@hrd.edu.vn
        
        VIETNAM HUMAN RESOURCES PROFESSIONAL
        - Hotline: :telephone_receiver:  0905 69 89 96 
        - :incoming_envelope:  chung@nghenhansu.vn
    """)
    st.markdown(f"""    
        ---
        # FOLLOW US: :thumbsup:
    """)
    url1 = "https://lnkd.in/gA5xeik"
    st.markdown("       1-Facebook: [https://lnkd.in/gA5xeik](%s)" % url1)

    url2 = "https://lnkd.in/gkxwPSFA"
    st.markdown("       2-LinkedIn: [https://lnkd.in/gkxwPSFA](%s)" % url2)


    url3 = "nghetuyendung.com"
    st.markdown("       3-Recruitment, Training, and Coaching: [nghetuyendung.com](%s)" % url3)

    url4 = "https://zalo.me/nghenhansu"
    st.markdown("       4-HR News on Zalo Official Account:  [https://zalo.me/nghenhansu](%s)" % url4)
             
            
    st.markdown("********************************")           
    st.text("")



set_page_info()



# import time
# import streamlit as st
# from langchain.chains import RetrievalQA
# from langchain.llms import HuggingFaceHub
# from langchain.vectorstores import FAISS
# # prompts
# from langchain import PromptTemplate, LLMChain
# import textwrap

# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# class CFG:
#     # LLMs
#     model_name = 'mistralai/Mistral-7B-Instruct-v0.1'
#     temperature = 0.5
#     top_p = 0.95
#     repetition_penalty = 1.15
#     do_sample = True
#     max_new_tokens = 1024
#     num_return_sequences=1

#     # splitting
#     split_chunk_size = 800
#     split_overlap = 0
    
#     # embeddings
#     embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'

#     # similar passages
#     k = 5
    
#     # paths
#     Embeddings_path =  './faiss_hp/'

# llm = HuggingFaceHub(
#     repo_id = CFG.model_name,
#     model_kwargs={
#         "max_new_tokens": CFG.max_new_tokens,
#         "temperature": CFG.temperature,
#         "top_p": CFG.top_p,
#         "repetition_penalty": CFG.repetition_penalty,
#         "do_sample": CFG.do_sample,
#         "num_return_sequences": CFG.num_return_sequences        
#     },
#     huggingfacehub_api_token = st.secrets["hugging_api_key"]
# )
